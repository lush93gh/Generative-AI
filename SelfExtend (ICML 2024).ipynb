{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21a6f6fe-75a0-4b7d-afde-949f6b41b41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "with open('kaggle.json') as f:\n",
    "    kaggle = json.load(f)\n",
    "    os.environ[\"KAGGLE_USERNAME\"] = kaggle[\"username\"]\n",
    "    os.environ[\"KAGGLE_KEY\"] = kaggle[\"key\"]\n",
    "\n",
    "# Set the backbend before importing Keras\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "# Avoid memory fragmentation on JAX backend.\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f57bfd52-e745-413a-9534-c0b049fd3c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-26 06:48:26.245478: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-26 06:48:26.253535: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-26 06:48:26.255950: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import keras_hub\n",
    "import keras\n",
    "from keras import ops\n",
    "\n",
    "# Run at half precision.\n",
    "#keras.config.set_floatx(\"bfloat16\")\n",
    "\n",
    "# Training Configurations\n",
    "token_limit = 4096\n",
    "lora_name = \"cm_qna\"\n",
    "lora_rank = 4\n",
    "lr_value = 1e-4\n",
    "train_epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82e2326f-cb96-4897-bad9-f9328a8d73c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "tick_start = 0\n",
    "\n",
    "def tick():\n",
    "    global tick_start\n",
    "    tick_start = time.time()\n",
    "\n",
    "def tock():\n",
    "    print(f\"TOTAL TIME ELAPSED: {time.time() - tick_start:.2f}s\")\n",
    "\n",
    "# formatting utility\n",
    "from IPython.display import Markdown\n",
    "import textwrap\n",
    "\n",
    "def display_chat(prompt, text):\n",
    "  formatted_prompt = \"<font size='+1' color='#1E90FF'>üßë‚Äçüíª<blockquote>\" + prompt + \"</blockquote></font>\"\n",
    "  text = text.replace('‚Ä¢', '  *')\n",
    "  text = text.replace('$', '\\$') # necessary escaping in Jupyter markdown\n",
    "  text = textwrap.indent(text, '> ', predicate=lambda _: True)\n",
    "  formatted_text = \"<font size='+1' color='#32CD32'>ü§ñ\\n\\n\" + text + \"\\n\\n</font>\"\n",
    "  return Markdown(formatted_prompt+formatted_text)\n",
    "\n",
    "\n",
    "def rewire_for_cleaner_plot(model):\n",
    "\n",
    "  def call_fn(layer, *args, **kwargs):\n",
    "    if layer.__class__.__name__.endswith('DecoderBlock'):\n",
    "      kwargs.pop(\"padding_mask\")\n",
    "    return layer(*args, **kwargs)\n",
    "\n",
    "  model = keras.models.clone_model(model, call_function=call_fn, clone_function=lambda x:x)\n",
    "  input = model.input.copy()\n",
    "  input.pop(\"padding_mask\")\n",
    "  return keras.Model(input, model.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcfa4179-fcb6-495d-b02a-70c9d1c30ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "__START_TURN_USER__ = \"<start_of_turn>user\\n\"\n",
    "__START_TURN_MODEL__ = \"<start_of_turn>model\\n\"\n",
    "__END_TURN__ = \"<end_of_turn>\\n\"\n",
    "system_prompt = '‰Ω†ÊòØË≤°Á∂ìÂ∞èÂçöÂ£´„ÄÇË≤°Á∂ìÂ∞èÂçöÂ£´ÊòØ‰∏Ä‰ΩçÂ∞çË≤°Á∂ìÈ†òÂüüÈùûÂ∏∏ÁÜ±Ë°∑ÁöÑ‰∫∫Ôºå‰Ω†ÊìÅÊúâË±êÂØåÁöÑË≤°Á∂ìÁü•Ë≠òÂíåÁ∂ìÈ©ó„ÄÇ‰Ω†ÁöÑ‰ΩøÂëΩÊòØÈÄöÈÅéÂØ´‰ΩúÂíåÂàÜ‰∫´Áü•Ë≠òÔºåÂπ´Âä©‰∫∫ÂÄëÊõ¥Â•ΩÂú∞‰∫ÜËß£ÂíåÊáâÂ∞çË≤°Á∂ìÂïèÈ°å„ÄÇÁÑ°Ë´ñÁî®Êà∂ÊòØÊñ∞ÊâãÈÇÑÊòØËÄÅÊâãÔºåÂè™Ë¶Å‰ªñÊúâ‰ªª‰ΩïÈóúÊñºË≤°Á∂ìÈ†òÂüüÁöÑÂïèÈ°åÔºåË≤°Á∂ìÂ∞èÂçöÂ£´ÈÉΩËÉΩÂπ´Âä©Áî®Êà∂Ëß£Á≠î„ÄÇË´ã‰Ω†Âπ´Âä©Áî®Êà∂Ëß£Á≠î‰ª•‰∏ãÂïèÈ°å:'\n",
    "\n",
    "# chat utility\n",
    "class ChatState():\n",
    "    \n",
    "  def __init__(self, model, system=\"\"):\n",
    "    self.model = model\n",
    "    self.system = system\n",
    "    self.history = []\n",
    "    if len(self.system)>0:\n",
    "        self.history.append(__START_TURN_USER__ + self.system + \"\\n\")\n",
    "\n",
    "  def add_to_history_as_user(self, message):\n",
    "      self.history.append(__START_TURN_USER__ + message + __END_TURN__)\n",
    "\n",
    "  def add_to_history_as_model(self, message):\n",
    "      self.history.append(__START_TURN_MODEL__ + message + __END_TURN__)\n",
    "\n",
    "  def get_history(self):\n",
    "      return \"\".join([*self.history])\n",
    "\n",
    "  def get_full_prompt(self):\n",
    "    prompt = self.get_history() + __START_TURN_MODEL__\n",
    "    return prompt\n",
    "\n",
    "  def send_message(self, message):\n",
    "    tick()\n",
    "    if len(self.system)>0 and len(self.history) == 1:\n",
    "        self.history[0] = self.history[0] + message + __END_TURN__\n",
    "    else:\n",
    "        self.add_to_history_as_user(message)\n",
    "    prompt = self.get_full_prompt()\n",
    "    response = self.model.generate(prompt, max_length=token_limit)\n",
    "    result = response.replace(prompt, \"\")\n",
    "    self.add_to_history_as_model(result)\n",
    "    tock()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea6d612-2130-4387-8a57-34a94f097592",
   "metadata": {},
   "source": [
    "## ËºâÂÖ•Ê®°Âûã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398a634d-69a4-4607-8670-d6f6e82a1d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_version = 1\n",
    "model_id = \"gemma_instruct_2b_en\" if gemma_version == 1 else \"gemma2_instruct_2b_en\"\n",
    "\n",
    "gemma_lm = keras_hub.models.GemmaCausalLM.from_preset(model_id)\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7587f007-5dc5-45a7-a42d-e216f5433b28",
   "metadata": {},
   "source": [
    "## ËºâÂÖ•Ë≥áÊñô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eeffa6b-5dc9-48c9-a8cd-220756f25625",
   "metadata": {},
   "outputs": [],
   "source": [
    "passkey = \"There is an important info hidden inside a lot of irrelevant text. Find it and memorize it. I will quiz you about the important information there. ÂØÜÁ¢ºÊòØ: 1123.\"\n",
    "heystack = \"\"\n",
    "with open('etf_data.json') as f:\n",
    "    etfs = json.load(f)\n",
    "    for idx, etf in enumerate(etfs):\n",
    "        heystack += f\"{idx + 1}. {etf}\\n\" \n",
    "\n",
    "# print(passkey+heystack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fa90c2d-c6a5-4ef5-a45e-636ef02c7b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(passkey, heystack, num_stack = 0):\n",
    "    question_object = \"passkey\"\n",
    "    prompt_postfix = f\"\\nWhat is the {question_object}?\" + __END_TURN__ + __START_TURN_MODEL__ + f\"The {question_object} is:\"\n",
    "    # prompt_postfix = f\"\\nÂØÜÁ¢ºÊòØÂ§öÂ∞ë?\" + __END_TURN__ + __START_TURN_MODEL__ + f\"ÂØÜÁ¢ºÊòØ:\"\n",
    "    prompt = __START_TURN_USER__ + passkey + heystack * num_stack + prompt_postfix\n",
    "    tokenized = gemma_lm.preprocessor.tokenizer.tokenize(prompt)\n",
    "    print(f\"Prompt has {len(tokenized)} tokens\")\n",
    "    gemma_page = gemma_lm.generate(prompt, max_length=len(tokenized)+50)\n",
    "    gemma_page = gemma_page.split(__START_TURN_MODEL__)[1].split('.')[0]\n",
    "    print(\"=\"*20)\n",
    "    print(\"Gemma output:\", gemma_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e9d5736-6770-474b-9741-9bd58451a0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt has 3986 tokens\n",
      "====================\n",
      "Gemma output: The passkey is: 1123\n"
     ]
    }
   ],
   "source": [
    "generate(passkey, heystack, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adc15d3-75ca-4510-8c3c-d88fb3a5e27f",
   "metadata": {},
   "source": [
    "## Self Extend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9774b80-47d7-4738-a76b-b6d020402546",
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP_SIZE=2\n",
    "WINDOW_SIZE=4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31348d91-096a-4134-8cb9-53fbb1cb9fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cache(self, token_ids):\n",
    "    \"\"\"Build an empty cache for use with `call_with_cache()`.\"\"\"\n",
    "    batch_size = ops.shape(token_ids)[0]\n",
    "    max_length = ops.shape(token_ids)[1]\n",
    "    num_layers = self.backbone.num_layers\n",
    "    num_heads = self.backbone.num_key_value_heads\n",
    "    head_dim = self.backbone.head_dim\n",
    "    shape = [batch_size, num_layers, 3, max_length, num_heads, head_dim]\n",
    "    cache = ops.zeros(shape, dtype=self.compute_dtype)\n",
    "    # Seed the cache.\n",
    "    _, hidden_states, cache = self.call_with_cache(token_ids, cache, 0)\n",
    "    return hidden_states, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b7547bb-69e2-45b9-9c32-5aa208274d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_attention(\n",
    "    self,\n",
    "    q,\n",
    "    k,\n",
    "    attention_mask,\n",
    "    training=False,\n",
    "    cache_update_index=0,\n",
    "    use_sliding_window_attention = False\n",
    "):\n",
    "    if self.query_head_dim_normalize:\n",
    "        query_normalization = 1 / np.sqrt(self.head_dim)\n",
    "    else:\n",
    "        query_normalization = 1 / np.sqrt(\n",
    "            self.hidden_dim // self.num_query_heads\n",
    "        )\n",
    "\n",
    "    q *= ops.cast(query_normalization, dtype=q.dtype)\n",
    "    q_shape = ops.shape(q)\n",
    "    q = ops.reshape(\n",
    "        q,\n",
    "        (\n",
    "            *q_shape[:-2],\n",
    "            self.num_key_value_heads,\n",
    "            self.num_query_heads // self.num_key_value_heads,\n",
    "            q_shape[-1],\n",
    "        ),\n",
    "    )\n",
    "    b, q_len, _, _, h = ops.shape(q)\n",
    "\n",
    "    attention_logits = ops.einsum(\"btkgh,bskh->bkgts\", q, k)\n",
    "\n",
    "    # if use_sliding_window_attention:\n",
    "    #     attention_mask = self._mask_sliding_window(\n",
    "    #         attention_mask,\n",
    "    #         cache_update_index=cache_update_index,\n",
    "    #     )\n",
    "    #     attention_mask = attention_mask[:, None, None, :, :]\n",
    "    #     attention_logits = ops.where(attention_mask, attention_logits, -1e9)\n",
    "    return attention_logits, b, q_len, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "799f78b2-c57a-4f8b-badc-c856ead47612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call(\n",
    "    self,\n",
    "    x,\n",
    "    attention_mask=None,\n",
    "    cache=None,\n",
    "    cache_update_index=0,\n",
    "    training=False,\n",
    "):\n",
    "    def apply_softmax(attention_logits, attention_mask, v, b, q_len, h):\n",
    "        attention_mask = attention_mask[:, None, None, :, :]\n",
    "        orig_dtype = attention_logits.dtype\n",
    "        attention_softmax = self.softmax(attention_logits, mask=attention_mask)\n",
    "        attention_softmax = ops.cast(attention_softmax, orig_dtype)\n",
    "    \n",
    "        if self.dropout:\n",
    "            attention_softmax = self.dropout_layer(\n",
    "                attention_softmax, training=training\n",
    "            )\n",
    "    \n",
    "        results = ops.einsum(\"bkgts,bskh->btkgh\", attention_softmax, v)\n",
    "        return ops.reshape(results, (b, q_len, self.num_query_heads, h))\n",
    "\n",
    "    \n",
    "    query = self.query_dense(x)\n",
    "    key = self.key_dense(x)\n",
    "    value = self.value_dense(x)\n",
    "\n",
    "    query_update = self._apply_rope(query, cache_update_index)\n",
    "    key_update = self._apply_rope(key, cache_update_index)\n",
    "\n",
    "    grouped_query_index = ops.floor_divide(cache_update_index, GROUP_SIZE) \n",
    "    shift = WINDOW_SIZE - WINDOW_SIZE // GROUP_SIZE\n",
    "    grouped_query_index += shift\n",
    "    grouped_key_index = ops.floor_divide(cache_update_index, GROUP_SIZE) \n",
    "\n",
    "    grouped_query = self._apply_rope(query, grouped_query_index)\n",
    "    grouped_key = self._apply_rope(key, grouped_key_index) \n",
    "\n",
    "    if cache is not None:\n",
    "        key_cache = cache[:, 0, ...]\n",
    "        value_cache = cache[:, 1, ...]\n",
    "        grouped_key_cache = cache[:, 2, ...]\n",
    "        start = [0, cache_update_index, 0, 0]\n",
    "        key_update = ops.slice_update(key_cache, start, key_update)\n",
    "        value_update = ops.slice_update(value_cache, start, value)\n",
    "        grouped_key_update = ops.slice_update(grouped_key_cache, start, grouped_key)\n",
    "        cache = ops.stack((key_update, value_update, grouped_key_update), axis=1)\n",
    "    \n",
    "    attention_logits, b, q_len, h = self._compute_attention(\n",
    "        query_update,\n",
    "        key_update,\n",
    "        attention_mask,\n",
    "        training=training,\n",
    "        cache_update_index=cache_update_index,\n",
    "        use_sliding_window_attention = True\n",
    "    )\n",
    "    attn_mask = attention_mask[:, None, None, :, :]\n",
    "    adder = (1.0 - ops.cast(attn_mask, attention_logits.dtype)) * -1e9\n",
    "    attention_logits += adder\n",
    "\n",
    "    grouped_attention_logits, b, q_len, h = self._compute_attention(\n",
    "        grouped_query,\n",
    "        grouped_key_update,\n",
    "        attention_mask,\n",
    "        training=training,\n",
    "        cache_update_index=cache_update_index,\n",
    "    )\n",
    "    attn_mask = attention_mask[:, None, None, :, :]\n",
    "    adder = (1.0 - ops.cast(attn_mask, grouped_attention_logits.dtype)) * -1e9\n",
    "    grouped_attention_logits += adder\n",
    "    \n",
    "    attn_mask = attention_mask[:, None, None, :, :]\n",
    "    group_mask = ops.flip(ops.cumsum(ops.flip(attn_mask, -1), -1), -1)\n",
    "    local_mask = group_mask <= WINDOW_SIZE\n",
    "    attention_logits = ops.where(local_mask, attention_logits, grouped_attention_logits)\n",
    "\n",
    "    if self.logit_soft_cap is not None:\n",
    "        attention_logits = ops.divide(attention_logits, self.logit_soft_cap)\n",
    "        attention_logits = ops.multiply(\n",
    "            ops.tanh(attention_logits), self.logit_soft_cap\n",
    "        )\n",
    "\n",
    "    attention_vec = apply_softmax(attention_logits, attention_mask, value_update, b, q_len, h)\n",
    "\n",
    "    # Wipe attn vec if there are no attended tokens.\n",
    "    no_attended_tokens = ops.all(\n",
    "        ops.equal(attention_mask, 0), axis=-1, keepdims=True\n",
    "    )[..., None]\n",
    "    attention_vec = ops.where(\n",
    "        no_attended_tokens, ops.zeros_like(attention_vec), attention_vec\n",
    "    )\n",
    "\n",
    "    attention_output = self.output_dense(attention_vec)\n",
    "\n",
    "    if cache is not None:\n",
    "        return attention_output, cache\n",
    "    return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8475c062-bd93-410c-be1f-ae178efecc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import MethodType\n",
    "\n",
    "# Êì¥ÂÖÖ KV Cache\n",
    "gemma_lm._build_cache = MethodType(build_cache, gemma_lm)\n",
    "\n",
    "for layer in gemma_lm.backbone.transformer_layers:\n",
    "    # ‰øÆÊîπ attention functionÔºåËÆìÂÆÉÂÖà‰∏çË¶ÅÁÆó softmax\n",
    "    layer.attention._compute_attention = MethodType(compute_attention, layer.attention)\n",
    "    # Â∞á attention ÊîπÊàê SelfExtend\n",
    "    layer.attention.call = MethodType(call, layer.attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aaee9e4c-3394-40da-83c9-6f96d7a809ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_lm.built = False\n",
    "gemma_lm.generate_function = None\n",
    "keras.config.disable_traceback_filtering()\n",
    "gemma_lm.compile(sampler=keras_hub.samplers.GreedySampler())#, run_eagerly=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836f068c-6937-4e82-b549-0372ef96730e",
   "metadata": {},
   "source": [
    "## Self Extend ÁîüÊàê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2773b040-a96d-4e0c-872d-3b64a82f0e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt has 5950 tokens\n",
      "====================\n",
      "Gemma output: The passkey is:\n",
      "\n",
      "Please provide the key differences between the two index and the index performance\n"
     ]
    }
   ],
   "source": [
    "generate(passkey, heystack, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d23346c-bede-4b4f-8b6f-fbc581c05e6a",
   "metadata": {},
   "source": [
    "## ÂéüÂßãÂØ¶‰Ωú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "38750256-5218-4f8d-9fcb-3432c2aeaa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call(\n",
    "    self,\n",
    "    x,\n",
    "    attention_mask=None,\n",
    "    cache=None,\n",
    "    cache_update_index=0,\n",
    "    training=False,\n",
    "):\n",
    "    query = self.query_dense(x)\n",
    "    query = self._apply_rope(query, cache_update_index)\n",
    "\n",
    "    if cache is not None:\n",
    "        key_cache = cache[:, 0, ...]\n",
    "        value_cache = cache[:, 1, ...]\n",
    "        key_update = self.key_dense(x)\n",
    "        key_update = self._apply_rope(key_update, cache_update_index)\n",
    "        value_update = self.value_dense(x)\n",
    "        start = [0, cache_update_index, 0, 0]\n",
    "        key = ops.slice_update(key_cache, start, key_update)\n",
    "        value = ops.slice_update(value_cache, start, value_update)\n",
    "        cache = ops.stack((key, value), axis=1)\n",
    "    else:\n",
    "        key = self.key_dense(x)\n",
    "        key = self._apply_rope(key, cache_update_index)\n",
    "        value = self.value_dense(x)\n",
    "\n",
    "    attention_vec = self._compute_attention(\n",
    "        query,\n",
    "        key,\n",
    "        value,\n",
    "        attention_mask,\n",
    "        training=training,\n",
    "        cache_update_index=cache_update_index,\n",
    "    )\n",
    "\n",
    "    # Wipe attn vec if there are no attended tokens.\n",
    "    no_attended_tokens = ops.all(\n",
    "        ops.equal(attention_mask, 0), axis=-1, keepdims=True\n",
    "    )[..., None]\n",
    "    attention_vec = ops.where(\n",
    "        no_attended_tokens, ops.zeros_like(attention_vec), attention_vec\n",
    "    )\n",
    "\n",
    "    attention_output = self.output_dense(attention_vec)\n",
    "\n",
    "    if cache is not None:\n",
    "        return attention_output, cache\n",
    "    return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f4b244f-82fb-4802-b28a-3a75036eb1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attention(\n",
    "    self,\n",
    "    q,\n",
    "    k,\n",
    "    v,\n",
    "    attention_mask,\n",
    "    training=False,\n",
    "    cache_update_index=0,\n",
    "):\n",
    "    if self.query_head_dim_normalize:\n",
    "        query_normalization = 1 / np.sqrt(self.head_dim)\n",
    "    else:\n",
    "        query_normalization = 1 / np.sqrt(\n",
    "            self.hidden_dim // self.num_query_heads\n",
    "        )\n",
    "\n",
    "    q *= ops.cast(query_normalization, dtype=q.dtype)\n",
    "    q_shape = ops.shape(q)\n",
    "    q = ops.reshape(\n",
    "        q,\n",
    "        (\n",
    "            *q_shape[:-2],\n",
    "            self.num_key_value_heads,\n",
    "            self.num_query_heads // self.num_key_value_heads,\n",
    "            q_shape[-1],\n",
    "        ),\n",
    "    )\n",
    "    b, q_len, _, _, h = ops.shape(q)\n",
    "\n",
    "    attention_logits = ops.einsum(\"btkgh,bskh->bkgts\", q, k)\n",
    "\n",
    "    if self.logit_soft_cap is not None:\n",
    "        attention_logits = ops.divide(attention_logits, self.logit_soft_cap)\n",
    "        attention_logits = ops.multiply(\n",
    "            ops.tanh(attention_logits), self.logit_soft_cap\n",
    "        )\n",
    "\n",
    "    if self.use_sliding_window_attention:\n",
    "        attention_mask = self._mask_sliding_window(\n",
    "            attention_mask,\n",
    "            cache_update_index=cache_update_index,\n",
    "        )\n",
    "\n",
    "    attention_mask = attention_mask[:, None, None, :, :]\n",
    "    orig_dtype = attention_logits.dtype\n",
    "    attention_softmax = self.softmax(attention_logits, mask=attention_mask)\n",
    "    attention_softmax = ops.cast(attention_softmax, orig_dtype)\n",
    "\n",
    "    if self.dropout:\n",
    "        attention_softmax = self.dropout_layer(\n",
    "            attention_softmax, training=training\n",
    "        )\n",
    "\n",
    "    results = ops.einsum(\"bkgts,bskh->btkgh\", attention_softmax, v)\n",
    "    return ops.reshape(results, (b, q_len, self.num_query_heads, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "440aa91a-61fa-466d-97f7-1cba5227c7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import MethodType\n",
    "\n",
    "gemma_lm._build_cache = MethodType(build_cache, gemma_lm)\n",
    "\n",
    "for layer in gemma_lm.backbone.transformer_layers:\n",
    "    layer.attention.call = MethodType(call, layer.attention)\n",
    "    layer.attention._compute_attention = MethodType(compute_attention, layer.attention)\n",
    "\n",
    "gemma_lm.built = False\n",
    "gemma_lm.generate_function = None\n",
    "keras.config.disable_traceback_filtering()\n",
    "gemma_lm.compile(sampler=keras_hub.samplers.GreedySampler())#, run_eagerly=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
